{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jk673/grapinnformer/blob/main/test_gnn_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7d569e1",
      "metadata": {
        "id": "f7d569e1"
      },
      "source": [
        "## Import packages and modules"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvista\n",
        "!pip install torch_geometric\n",
        "!pip install huggingface_hub\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MvBJAC24MOoH",
        "outputId": "c18f07b1-73d1-4102-9621-99f3348cb307"
      },
      "id": "MvBJAC24MOoH",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvista\n",
            "  Downloading pyvista-0.46.1-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: matplotlib>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from pyvista) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from pyvista) (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from pyvista) (11.3.0)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.11/dist-packages (from pyvista) (1.8.2)\n",
            "Requirement already satisfied: scooby>=0.5.1 in /usr/local/lib/python3.11/dist-packages (from pyvista) (0.10.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10 in /usr/local/lib/python3.11/dist-packages (from pyvista) (4.14.1)\n",
            "Collecting vtk!=9.4.0 (from pyvista)\n",
            "  Downloading vtk-9.5.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.1->pyvista) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.1->pyvista) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.1->pyvista) (4.59.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.1->pyvista) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.1->pyvista) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.1->pyvista) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.0.1->pyvista) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch->pyvista) (4.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch->pyvista) (2.32.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.0.1->pyvista) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch->pyvista) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch->pyvista) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch->pyvista) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch->pyvista) (2025.8.3)\n",
            "Downloading pyvista-0.46.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading vtk-9.5.0-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (112.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.1/112.1 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: vtk, pyvista\n",
            "Successfully installed pyvista-0.46.1 vtk-9.5.0\n",
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.12.15)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (3.2.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch_geometric) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch_geometric) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch_geometric) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch_geometric) (2025.8.3)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from aiosignal>=1.4.0->aiohttp->torch_geometric) (4.14.1)\n",
            "Downloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, os, sys, subprocess\n",
        "\n",
        "torch_ver = torch.__version__.split('+')[0]           # e.g. '2.6.0'\n",
        "cuda_ver  = torch.version.cuda or ''\n",
        "cu_tag    = f\"cu{cuda_ver.replace('.','')}\" if torch.cuda.is_available() and cuda_ver else \"cpu\"\n",
        "url = f\"https://data.pyg.org/whl/torch-{torch_ver}+{cu_tag}.html\"\n",
        "\n",
        "print(\"Torch:\", torch.__version__, \"| CUDA:\", cuda_ver or \"cpu\", \"| PYG wheel index:\", url)\n",
        "\n",
        "# 설치\n",
        "subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-U\",\n",
        "    \"pyg-lib\", \"torch-scatter\", \"torch-sparse\", \"torch-cluster\", \"torch-spline-conv\", \"-f\", url\n",
        "])\n",
        "\n",
        "print(\"✅ Done. Please restart runtime/kernel, then re-run your code.\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaZ23pG-yD-D",
        "outputId": "7c94c150-59fd-454d-e4ea-93020108cce9"
      },
      "id": "MaZ23pG-yD-D",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.34.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.7)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.8.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pyg_lib, torch_sparse\n",
        "print(\"pyg-lib ok;\", getattr(pyg_lib, \"__version__\", \"ok\"))\n",
        "print(\"torch-sparse ok;\", getattr(torch_sparse, \"__version__\", \"ok\"))\n"
      ],
      "metadata": {
        "id": "ZOUBcCyPLnL4"
      },
      "id": "ZOUBcCyPLnL4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b647a6a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b647a6a0",
        "outputId": "37cb7c4d-58e5-4928-cf72-af9ac46f435d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Torch: 2.6.0+cu124\n",
            "CUDA: 12.4\n",
            "PyVista: 0.46.1\n",
            "PyG: 2.6.1\n",
            "py  : 3.11.13\n"
          ]
        }
      ],
      "source": [
        "# 1. Environment & dependencies\n",
        "import os\n",
        "import glob\n",
        "import sys\n",
        "import random\n",
        "import math\n",
        "from pathlib import Path\n",
        "import time\n",
        "import gc\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pyvista as pv\n",
        "import torch_geometric\n",
        "from torch_geometric.data import Data as GeoData, Batch\n",
        "from torch_geometric.loader import DataLoader as GeoLoader, NeighborLoader\n",
        "from torch_geometric.nn import GCNConv, TransformerConv, JumpingKnowledge, GraphNorm\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "import wandb\n",
        "\n",
        "# Data and model settings\n",
        "DATA_ROOT = Path('data')\n",
        "TARGET_FIELD = 'static(p)_coeffMean'\n",
        "USE_NORMALS = True\n",
        "\n",
        "# Reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "\n",
        "print('Torch:', torch.__version__)\n",
        "print('CUDA:', torch.version.cuda)\n",
        "print('PyVista:', pv.__version__)\n",
        "print('PyG:', torch_geometric.__version__ if 'torch_geometric' in sys.modules else 'unknown')\n",
        "print(\"py  :\", sys.version.split()[0])  # ex) 3.11.x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29f66f93",
      "metadata": {
        "id": "29f66f93"
      },
      "source": [
        "## Load Graphs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import torch\n",
        "\n",
        "# Hugging Face에 올린 repo 정보 입력\n",
        "# (dataset에 올렸으면 repo_type=\"dataset\", model repo면 repo_type=\"model\")\n",
        "file_path = hf_hub_download(\n",
        "    repo_id=\"chunsamkim/ahmed_graphs\",   # 본인 repo 경로\n",
        "    filename=\"graphs_cache.pt\",      # 저장한 .pt 파일 이름\n",
        "    repo_type=\"dataset\"                  # dataset repo면 꼭 지정\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSAOUaB21uIt",
        "outputId": "05babf7c-8214-4e91-c455-9ac30dd46e05"
      },
      "id": "sSAOUaB21uIt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# torch로 로드\n",
        "model_state = torch.load(file_path, weights_only=False)\n",
        "print(model_state)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssAmSwyU8eQB",
        "outputId": "63ea9ed3-30a3-4139-9778-4b415b0d2c9c"
      },
      "id": "ssAmSwyU8eQB",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([Data(x=[1284274, 8], edge_index=[2, 7575604], y=[1284274, 1], y_graph=[2]), Data(x=[986845, 8], edge_index=[2, 5817912], y=[986845, 1], y_graph=[2]), Data(x=[1127016, 8], edge_index=[2, 6644564], y=[1127016, 1], y_graph=[2]), Data(x=[1299162, 8], edge_index=[2, 7662868], y=[1299162, 1], y_graph=[2]), Data(x=[1192424, 8], edge_index=[2, 7031024], y=[1192424, 1], y_graph=[2]), Data(x=[785808, 8], edge_index=[2, 4629838], y=[785808, 1], y_graph=[2]), Data(x=[924044, 8], edge_index=[2, 5445480], y=[924044, 1], y_graph=[2]), Data(x=[1241558, 8], edge_index=[2, 7319802], y=[1241558, 1], y_graph=[2]), Data(x=[912689, 8], edge_index=[2, 5377876], y=[912689, 1], y_graph=[2]), Data(x=[1086517, 8], edge_index=[2, 6406878], y=[1086517, 1], y_graph=[2]), Data(x=[1101032, 8], edge_index=[2, 6491354], y=[1101032, 1], y_graph=[2]), Data(x=[1109704, 8], edge_index=[2, 6541506], y=[1109704, 1], y_graph=[2]), Data(x=[954647, 8], edge_index=[2, 5623508], y=[954647, 1], y_graph=[2]), Data(x=[1106471, 8], edge_index=[2, 6521932], y=[1106471, 1], y_graph=[2]), Data(x=[1139498, 8], edge_index=[2, 6709934], y=[1139498, 1], y_graph=[2]), Data(x=[1085421, 8], edge_index=[2, 6399558], y=[1085421, 1], y_graph=[2]), Data(x=[962658, 8], edge_index=[2, 5673830], y=[962658, 1], y_graph=[2]), Data(x=[1125935, 8], edge_index=[2, 6638486], y=[1125935, 1], y_graph=[2]), Data(x=[1412811, 8], edge_index=[2, 8335456], y=[1412811, 1], y_graph=[2]), Data(x=[827656, 8], edge_index=[2, 4874998], y=[827656, 1], y_graph=[2]), Data(x=[1026706, 8], edge_index=[2, 6052192], y=[1026706, 1], y_graph=[2]), Data(x=[1044634, 8], edge_index=[2, 6159922], y=[1044634, 1], y_graph=[2]), Data(x=[1125390, 8], edge_index=[2, 6635136], y=[1125390, 1], y_graph=[2]), Data(x=[1023210, 8], edge_index=[2, 6030270], y=[1023210, 1], y_graph=[2]), Data(x=[1116030, 8], edge_index=[2, 6579774], y=[1116030, 1], y_graph=[2]), Data(x=[1262093, 8], edge_index=[2, 7443664], y=[1262093, 1], y_graph=[2]), Data(x=[947180, 8], edge_index=[2, 5582970], y=[947180, 1], y_graph=[2]), Data(x=[992845, 8], edge_index=[2, 5851570], y=[992845, 1], y_graph=[2]), Data(x=[874742, 8], edge_index=[2, 5155322], y=[874742, 1], y_graph=[2]), Data(x=[1085641, 8], edge_index=[2, 6397058], y=[1085641, 1], y_graph=[2]), Data(x=[1170419, 8], edge_index=[2, 6899944], y=[1170419, 1], y_graph=[2]), Data(x=[1165571, 8], edge_index=[2, 6874724], y=[1165571, 1], y_graph=[2]), Data(x=[972467, 8], edge_index=[2, 5731924], y=[972467, 1], y_graph=[2]), Data(x=[1207932, 8], edge_index=[2, 7123498], y=[1207932, 1], y_graph=[2]), Data(x=[1086394, 8], edge_index=[2, 6402858], y=[1086394, 1], y_graph=[2]), Data(x=[911663, 8], edge_index=[2, 5374270], y=[911663, 1], y_graph=[2]), Data(x=[1092106, 8], edge_index=[2, 6440864], y=[1092106, 1], y_graph=[2]), Data(x=[1004232, 8], edge_index=[2, 5920658], y=[1004232, 1], y_graph=[2]), Data(x=[1241803, 8], edge_index=[2, 7324270], y=[1241803, 1], y_graph=[2]), Data(x=[1087017, 8], edge_index=[2, 6407264], y=[1087017, 1], y_graph=[2]), Data(x=[949780, 8], edge_index=[2, 5596020], y=[949780, 1], y_graph=[2]), Data(x=[976510, 8], edge_index=[2, 5754944], y=[976510, 1], y_graph=[2]), Data(x=[1058466, 8], edge_index=[2, 6239584], y=[1058466, 1], y_graph=[2]), Data(x=[1266226, 8], edge_index=[2, 7465284], y=[1266226, 1], y_graph=[2]), Data(x=[1298815, 8], edge_index=[2, 7660650], y=[1298815, 1], y_graph=[2]), Data(x=[1188793, 8], edge_index=[2, 7010054], y=[1188793, 1], y_graph=[2]), Data(x=[1138721, 8], edge_index=[2, 6711768], y=[1138721, 1], y_graph=[2]), Data(x=[836180, 8], edge_index=[2, 4927464], y=[836180, 1], y_graph=[2]), Data(x=[1093053, 8], edge_index=[2, 6443326], y=[1093053, 1], y_graph=[2]), Data(x=[876061, 8], edge_index=[2, 5160462], y=[876061, 1], y_graph=[2]), Data(x=[818693, 8], edge_index=[2, 4822434], y=[818693, 1], y_graph=[2]), Data(x=[881882, 8], edge_index=[2, 5196026], y=[881882, 1], y_graph=[2]), Data(x=[1292868, 8], edge_index=[2, 7625648], y=[1292868, 1], y_graph=[2]), Data(x=[1022589, 8], edge_index=[2, 6027168], y=[1022589, 1], y_graph=[2]), Data(x=[1164162, 8], edge_index=[2, 6864980], y=[1164162, 1], y_graph=[2]), Data(x=[1122186, 8], edge_index=[2, 6617122], y=[1122186, 1], y_graph=[2]), Data(x=[995748, 8], edge_index=[2, 5869342], y=[995748, 1], y_graph=[2]), Data(x=[1211123, 8], edge_index=[2, 7142654], y=[1211123, 1], y_graph=[2]), Data(x=[1287151, 8], edge_index=[2, 7589712], y=[1287151, 1], y_graph=[2]), Data(x=[954277, 8], edge_index=[2, 5624840], y=[954277, 1], y_graph=[2]), Data(x=[1215780, 8], edge_index=[2, 7170090], y=[1215780, 1], y_graph=[2]), Data(x=[1082317, 8], edge_index=[2, 6380654], y=[1082317, 1], y_graph=[2]), Data(x=[1050167, 8], edge_index=[2, 6191342], y=[1050167, 1], y_graph=[2]), Data(x=[1419752, 8], edge_index=[2, 8375364], y=[1419752, 1], y_graph=[2]), Data(x=[822562, 8], edge_index=[2, 4847634], y=[822562, 1], y_graph=[2]), Data(x=[1193657, 8], edge_index=[2, 7040556], y=[1193657, 1], y_graph=[2]), Data(x=[1002112, 8], edge_index=[2, 5908220], y=[1002112, 1], y_graph=[2]), Data(x=[1050041, 8], edge_index=[2, 6189148], y=[1050041, 1], y_graph=[2]), Data(x=[1108744, 8], edge_index=[2, 6537790], y=[1108744, 1], y_graph=[2]), Data(x=[1268335, 8], edge_index=[2, 7480236], y=[1268335, 1], y_graph=[2]), Data(x=[1163416, 8], edge_index=[2, 6858746], y=[1163416, 1], y_graph=[2]), Data(x=[1158935, 8], edge_index=[2, 6835722], y=[1158935, 1], y_graph=[2]), Data(x=[1010894, 8], edge_index=[2, 5957564], y=[1010894, 1], y_graph=[2]), Data(x=[1072416, 8], edge_index=[2, 6322926], y=[1072416, 1], y_graph=[2]), Data(x=[973489, 8], edge_index=[2, 5737604], y=[973489, 1], y_graph=[2]), Data(x=[879150, 8], edge_index=[2, 5180086], y=[879150, 1], y_graph=[2]), Data(x=[1034176, 8], edge_index=[2, 6093576], y=[1034176, 1], y_graph=[2]), Data(x=[902222, 8], edge_index=[2, 5316956], y=[902222, 1], y_graph=[2]), Data(x=[1463582, 8], edge_index=[2, 8631720], y=[1463582, 1], y_graph=[2]), Data(x=[1113095, 8], edge_index=[2, 6562660], y=[1113095, 1], y_graph=[2]), Data(x=[1063676, 8], edge_index=[2, 6268214], y=[1063676, 1], y_graph=[2]), Data(x=[1122253, 8], edge_index=[2, 6618764], y=[1122253, 1], y_graph=[2]), Data(x=[851533, 8], edge_index=[2, 5016424], y=[851533, 1], y_graph=[2]), Data(x=[881722, 8], edge_index=[2, 5194392], y=[881722, 1], y_graph=[2]), Data(x=[1089599, 8], edge_index=[2, 6424124], y=[1089599, 1], y_graph=[2]), Data(x=[1064215, 8], edge_index=[2, 6273694], y=[1064215, 1], y_graph=[2]), Data(x=[1066141, 8], edge_index=[2, 6284078], y=[1066141, 1], y_graph=[2]), Data(x=[1131049, 8], edge_index=[2, 6668982], y=[1131049, 1], y_graph=[2]), Data(x=[1408647, 8], edge_index=[2, 8311306], y=[1408647, 1], y_graph=[2]), Data(x=[1015427, 8], edge_index=[2, 5985910], y=[1015427, 1], y_graph=[2]), Data(x=[1053063, 8], edge_index=[2, 6209396], y=[1053063, 1], y_graph=[2]), Data(x=[1065423, 8], edge_index=[2, 6282450], y=[1065423, 1], y_graph=[2]), Data(x=[1147468, 8], edge_index=[2, 6764020], y=[1147468, 1], y_graph=[2]), Data(x=[1082137, 8], edge_index=[2, 6379738], y=[1082137, 1], y_graph=[2]), Data(x=[1196801, 8], edge_index=[2, 7056636], y=[1196801, 1], y_graph=[2]), Data(x=[1179290, 8], edge_index=[2, 6953160], y=[1179290, 1], y_graph=[2]), Data(x=[1011770, 8], edge_index=[2, 5960590], y=[1011770, 1], y_graph=[2]), Data(x=[952260, 8], edge_index=[2, 5613850], y=[952260, 1], y_graph=[2]), Data(x=[825478, 8], edge_index=[2, 4865592], y=[825478, 1], y_graph=[2]), Data(x=[757828, 8], edge_index=[2, 4465252], y=[757828, 1], y_graph=[2]), Data(x=[932468, 8], edge_index=[2, 5496006], y=[932468, 1], y_graph=[2]), Data(x=[1110592, 8], edge_index=[2, 6544180], y=[1110592, 1], y_graph=[2]), Data(x=[1063080, 8], edge_index=[2, 6265826], y=[1063080, 1], y_graph=[2]), Data(x=[869750, 8], edge_index=[2, 5126034], y=[869750, 1], y_graph=[2]), Data(x=[908995, 8], edge_index=[2, 5355336], y=[908995, 1], y_graph=[2]), Data(x=[844269, 8], edge_index=[2, 4973050], y=[844269, 1], y_graph=[2]), Data(x=[1070584, 8], edge_index=[2, 6309450], y=[1070584, 1], y_graph=[2]), Data(x=[1032781, 8], edge_index=[2, 6087772], y=[1032781, 1], y_graph=[2]), Data(x=[1143770, 8], edge_index=[2, 6744170], y=[1143770, 1], y_graph=[2]), Data(x=[1232368, 8], edge_index=[2, 7266884], y=[1232368, 1], y_graph=[2]), Data(x=[1388515, 8], edge_index=[2, 8193178], y=[1388515, 1], y_graph=[2]), Data(x=[1001460, 8], edge_index=[2, 5901776], y=[1001460, 1], y_graph=[2]), Data(x=[1005559, 8], edge_index=[2, 5929532], y=[1005559, 1], y_graph=[2]), Data(x=[1160763, 8], edge_index=[2, 6846022], y=[1160763, 1], y_graph=[2]), Data(x=[1107660, 8], edge_index=[2, 6530768], y=[1107660, 1], y_graph=[2]), Data(x=[1251539, 8], edge_index=[2, 7380142], y=[1251539, 1], y_graph=[2]), Data(x=[993334, 8], edge_index=[2, 5852842], y=[993334, 1], y_graph=[2]), Data(x=[1122501, 8], edge_index=[2, 6620146], y=[1122501, 1], y_graph=[2]), Data(x=[1521176, 8], edge_index=[2, 8972962], y=[1521176, 1], y_graph=[2]), Data(x=[1051136, 8], edge_index=[2, 6197034], y=[1051136, 1], y_graph=[2])], [Data(x=[1174908, 8], edge_index=[2, 6928266], y=[1174908, 1], y_graph=[2]), Data(x=[1113616, 8], edge_index=[2, 6564284], y=[1113616, 1], y_graph=[2]), Data(x=[916889, 8], edge_index=[2, 5405568], y=[916889, 1], y_graph=[2]), Data(x=[1341809, 8], edge_index=[2, 7912556], y=[1341809, 1], y_graph=[2]), Data(x=[1160205, 8], edge_index=[2, 6841322], y=[1160205, 1], y_graph=[2]), Data(x=[908354, 8], edge_index=[2, 5352488], y=[908354, 1], y_graph=[2]), Data(x=[952723, 8], edge_index=[2, 5616160], y=[952723, 1], y_graph=[2]), Data(x=[1217215, 8], edge_index=[2, 7176510], y=[1217215, 1], y_graph=[2]), Data(x=[1113517, 8], edge_index=[2, 6564992], y=[1113517, 1], y_graph=[2]), Data(x=[1180623, 8], edge_index=[2, 6964294], y=[1180623, 1], y_graph=[2]), Data(x=[1153168, 8], edge_index=[2, 6800442], y=[1153168, 1], y_graph=[2]), Data(x=[1038726, 8], edge_index=[2, 6124006], y=[1038726, 1], y_graph=[2]), Data(x=[1282052, 8], edge_index=[2, 7558272], y=[1282052, 1], y_graph=[2]), Data(x=[1028923, 8], edge_index=[2, 6063944], y=[1028923, 1], y_graph=[2]), Data(x=[802341, 8], edge_index=[2, 4726434], y=[802341, 1], y_graph=[2]), Data(x=[930917, 8], edge_index=[2, 5486766], y=[930917, 1], y_graph=[2]), Data(x=[912688, 8], edge_index=[2, 5379656], y=[912688, 1], y_graph=[2]), Data(x=[818986, 8], edge_index=[2, 4826436], y=[818986, 1], y_graph=[2]), Data(x=[1199546, 8], edge_index=[2, 7072556], y=[1199546, 1], y_graph=[2]), Data(x=[1259998, 8], edge_index=[2, 7432136], y=[1259998, 1], y_graph=[2]), Data(x=[987153, 8], edge_index=[2, 5818380], y=[987153, 1], y_graph=[2]), Data(x=[1083605, 8], edge_index=[2, 6387884], y=[1083605, 1], y_graph=[2]), Data(x=[1452569, 8], edge_index=[2, 8568762], y=[1452569, 1], y_graph=[2]), Data(x=[1204436, 8], edge_index=[2, 7101854], y=[1204436, 1], y_graph=[2]), Data(x=[1051535, 8], edge_index=[2, 6199332], y=[1051535, 1], y_graph=[2]), Data(x=[1198225, 8], edge_index=[2, 7065440], y=[1198225, 1], y_graph=[2]), Data(x=[1328371, 8], edge_index=[2, 7831196], y=[1328371, 1], y_graph=[2]), Data(x=[949463, 8], edge_index=[2, 5594912], y=[949463, 1], y_graph=[2]), Data(x=[971543, 8], edge_index=[2, 5725166], y=[971543, 1], y_graph=[2]), Data(x=[1453719, 8], edge_index=[2, 8575634], y=[1453719, 1], y_graph=[2])])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e69f940",
      "metadata": {
        "id": "2e69f940"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "train_graphs, val_graphs = model_state\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7984125b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81,
          "referenced_widgets": [
            "ebf198a7c7224db78796dc992c254ad6",
            "7117738377624b258f5e0553bb076dde",
            "34d8a032ac2f493b8890e073361f217a",
            "720ff6029503454dbe17ce48ee185171",
            "c569f62b6ba44651bcd28795f55ff278",
            "91243b41602a42f78630f86badf6efe4",
            "490ce153e6b54745a063b6a2cc45493d",
            "2f187208954a48cbbff3c99eb648fb63",
            "891b6d2831144e4f8809440fb335cce7",
            "0d4e5df42d14457190dbae7d8e7849a3",
            "2c8f3cca2f0848be913cd4ee4adc91ce",
            "4b80160a83d649ba8e40e4e63b107971",
            "84724253da914c439a94850fd8b5497f",
            "b1f8d921ba3b47b48d45b4296f2c9909",
            "61dddc2a244d4b879cf4bd8d6f29adbb",
            "1089a4c9fe444710943be8e44aedbcf8",
            "fd04420e04814f1285d9213e0b2507f9",
            "96e29fbeb2d3469e84bad47d4fe4f2bb",
            "58acb190438341b2889dc1e6df9143b5",
            "8db388040b724920a8f164fe64dc00da",
            "86f1a8d258f74367b84a633441bdf99c",
            "dfa469128d644cd3aeb4f8bf890faaaf"
          ]
        },
        "id": "7984125b",
        "outputId": "e02f2294-c9b0-4b38-8a54-cef24003c896"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "to_pyg:   0%|          | 0/120 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ebf198a7c7224db78796dc992c254ad6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "to_pyg:   0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b80160a83d649ba8e40e4e63b107971"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from torch_geometric.data import Data\n",
        "\n",
        "\n",
        "def to_pyg(graphs):\n",
        "    out = []\n",
        "    for g in tqdm(graphs, desc='to_pyg'):\n",
        "        if isinstance(g, dict):\n",
        "            x = torch.tensor(g['x'], dtype=torch.float32)\n",
        "            edge_index = torch.tensor(g['edge_index'], dtype=torch.long)\n",
        "            y = torch.tensor(g['y'], dtype=torch.float32)\n",
        "            out.append(Data(x=x, edge_index=edge_index, y=y))\n",
        "        else:\n",
        "            out.append(g)\n",
        "    return out\n",
        "\n",
        "train_graphs = to_pyg(train_graphs)\n",
        "val_graphs   = to_pyg(val_graphs)\n",
        "\n",
        "# PyG Data로 변환된 뒤에 한 번만 실행\n",
        "for lst in (train_graphs, val_graphs):\n",
        "    for d in lst:\n",
        "        if getattr(d, 'pos', None) is None:\n",
        "            d.pos = d.x[:, :3].contiguous()   # x = [xyz,(normals...)]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63389fa1",
      "metadata": {
        "id": "63389fa1"
      },
      "source": [
        "## Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c80b58d6",
      "metadata": {
        "id": "c80b58d6"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import TransformerConv, JumpingKnowledge, GraphNorm\n",
        "import torch.utils.checkpoint as cp\n",
        "\n",
        "class FourierPosEnc(nn.Module):\n",
        "    def __init__(self, in_ch=3, num_frequencies=2):  # ★ pos 주파수 절반으로\n",
        "        super().__init__()\n",
        "        self.register_buffer(\"freqs\", 2.0**torch.arange(num_frequencies) * torch.pi)\n",
        "        self.out_dim = in_ch*(2*num_frequencies)\n",
        "    def forward(self, pos):\n",
        "        pe = []\n",
        "        for f in self.freqs:\n",
        "            ang = f * pos\n",
        "            pe += [torch.sin(ang), torch.cos(ang)]\n",
        "        return torch.cat(pe, dim=-1)\n",
        "\n",
        "class FiLM(nn.Module):\n",
        "    def __init__(self, cond_dim, hidden):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(cond_dim, hidden*2), nn.GELU(), nn.Linear(hidden*2, hidden*2)\n",
        "        )\n",
        "    def forward(self, h, cond):\n",
        "        if cond is None: return h\n",
        "        if cond.dim()==1: cond = cond.unsqueeze(0)\n",
        "        gamma, beta = torch.chunk(self.mlp(cond), 2, dim=-1)\n",
        "        return h * (1 + gamma.squeeze(0)) + beta.squeeze(0)\n",
        "\n",
        "class BoundaryGraphNet(nn.Module):\n",
        "    def __init__(self, in_dim, hidden=96, layers=4, dropout=0.15,   # ★ 192→96, 6→4\n",
        "                 heads=2, use_pos_enc=True, pos_freqs=2, cond_dim=4, # ★ 4→2\n",
        "                 out_dim_p=1, out_dim_tau=3, jk_mode='last', use_checkpoint=False,\n",
        "                 predict_tau=False):\n",
        "        super().__init__()\n",
        "        self.use_pos_enc = use_pos_enc\n",
        "        self.pos_enc = FourierPosEnc(3, pos_freqs) if use_pos_enc else None\n",
        "        self.predict_tau = predict_tau\n",
        "        pe_dim = self.pos_enc.out_dim if use_pos_enc else 0\n",
        "\n",
        "        self.x_enc = nn.Sequential(\n",
        "            nn.Linear(in_dim + pe_dim, hidden),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(hidden, hidden)\n",
        "        )\n",
        "\n",
        "        # ★ edge_dim을 8로 크게 축소\n",
        "        self.edge_encoder = nn.Sequential(\n",
        "            nn.Linear(4, 16), nn.GELU(), nn.Linear(16, 8)\n",
        "        )\n",
        "        self.edge_dim = 8\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.norms  = nn.ModuleList()\n",
        "        for _ in range(layers):\n",
        "            # ★ concat=False, out_channels=hidden, add_self_loops=False\n",
        "            self.layers.append(TransformerConv(\n",
        "                in_channels=hidden,\n",
        "                out_channels=hidden,\n",
        "                heads=heads,\n",
        "                concat=False,\n",
        "                dropout=dropout,\n",
        "                edge_dim=self.edge_dim,\n",
        "                beta=True,\n",
        "            ))\n",
        "            self.norms.append(GraphNorm(hidden))\n",
        "\n",
        "        # ★ JK를 'last'로 기본 설정(메모리 절약). 필요시 'max'로 바꿔도 됨\n",
        "        self.jk_mode = jk_mode\n",
        "        if jk_mode == 'max':\n",
        "            self.jk = JumpingKnowledge(mode='max')\n",
        "        else:\n",
        "            self.jk = None\n",
        "\n",
        "        self.p_head   = nn.Sequential(nn.Linear(hidden, hidden), nn.GELU(), nn.Dropout(dropout), nn.Linear(hidden, out_dim_p))\n",
        "        self.tau_head = nn.Sequential(nn.Linear(hidden, hidden), nn.GELU(), nn.Dropout(dropout), nn.Linear(hidden, out_dim_tau))\n",
        "\n",
        "        self.film = FiLM(cond_dim, hidden) if cond_dim>0 else None\n",
        "        self.dropout = dropout\n",
        "        self.use_checkpoint = use_checkpoint  # ★ 필요할 때만 켜기\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_edge_attr(pos, edge_index):\n",
        "        i, j = edge_index\n",
        "        rij = pos[j] - pos[i]\n",
        "        dij = torch.norm(rij, dim=1, keepdim=True).clamp_min(1e-12)\n",
        "        return torch.cat([rij, dij], dim=1)  # [E,4]\n",
        "\n",
        "    def forward(self, data):\n",
        "        x, pos, edge_index = data.x, data.pos, data.edge_index\n",
        "        pe = self.pos_enc(pos) if self.use_pos_enc else None\n",
        "        h = self.x_enc(torch.cat([x, pe], dim=-1) if pe is not None else x)\n",
        "\n",
        "        eattr = getattr(data, 'edge_attr', None)\n",
        "        if eattr is None:\n",
        "            eattr = self._make_edge_attr(pos, edge_index)\n",
        "        e = self.edge_encoder(eattr)\n",
        "\n",
        "        hs = [] if self.jk_mode == 'max' else None\n",
        "\n",
        "        for conv, norm in zip(self.layers, self.norms):\n",
        "            def block(h_in):\n",
        "                h_mid = conv(h_in, edge_index, edge_attr=e)\n",
        "                if self.film is not None and hasattr(data, 'global_cond'):\n",
        "                    h_mid = self.film(h_mid, data.global_cond)\n",
        "                h_mid = norm(h_mid)\n",
        "                h_mid = F.gelu(h_mid)\n",
        "                h_mid = F.dropout(h_mid, p=self.dropout, training=self.training)\n",
        "                return h_mid\n",
        "\n",
        "            h_res = h\n",
        "            # ★ 선택적 체크포인팅(메모리↓, 연산↑)\n",
        "            h = block(h) if not self.use_checkpoint else cp.checkpoint(block, h)\n",
        "            h = h + h_res\n",
        "            if hs is not None:\n",
        "                hs.append(h)\n",
        "\n",
        "        if self.jk_mode == 'max':\n",
        "            h = self.jk(hs)\n",
        "        # 'last'면 그냥 마지막 h 사용\n",
        "\n",
        "        p_pred   = self.p_head(h)\n",
        "        tau_pred = self.tau_head(h)\n",
        "        return (p_pred, tau_pred) if self.predict_tau else p_pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00f609ff",
      "metadata": {
        "id": "00f609ff"
      },
      "source": [
        "## Subgraph Seperation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### khop method"
      ],
      "metadata": {
        "id": "yfdMxYWALyi3"
      },
      "id": "yfdMxYWALyi3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b9ecfad",
      "metadata": {
        "id": "5b9ecfad"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.utils import k_hop_subgraph\n",
        "from torch_geometric.data import Data\n",
        "import torch, gc\n",
        "\n",
        "\n",
        "def make_khop_patches(g: Data, k=2, seeds_per_patch=1024, min_nodes=4096, max_patches=None):\n",
        "    N = g.num_nodes\n",
        "    device = g.edge_index.device\n",
        "    patches = []\n",
        "    visited = torch.zeros(N, dtype=torch.bool, device=device)\n",
        "\n",
        "    # 아직 안 담긴 노드들 중에서 씨드 묶음을 뽑아 k-hop 확장 → 패치 하나\n",
        "    while (~visited).any():\n",
        "        # 씨드: 남은 노드에서 무작위/순차 선택 (여기선 순차)\n",
        "        seeds = (~visited).nonzero(as_tuple=False).view(-1)[:seeds_per_patch]\n",
        "        if seeds.numel() == 0:\n",
        "            break\n",
        "\n",
        "        subset, sub_ei, _, edge_mask = k_hop_subgraph(\n",
        "            node_idx=seeds, num_hops=k, edge_index=g.edge_index,\n",
        "            relabel_nodes=True, num_nodes=N\n",
        "        )\n",
        "\n",
        "        if subset.numel() < min_nodes:\n",
        "            # 너무 작으면 한 번 더 확대: 씨드 수를 늘리거나 k를 키워도 됨\n",
        "            # 간단히: 남은 노드 더 합쳐서 다시 시도\n",
        "            extra = (~visited).nonzero(as_tuple=False).view(-1)[:seeds_per_patch*2]\n",
        "            if extra.numel() == 0:\n",
        "                break\n",
        "            subset, sub_ei, _, edge_mask = k_hop_subgraph(\n",
        "                node_idx=torch.unique(torch.cat([seeds, extra])),\n",
        "                num_hops=k, edge_index=g.edge_index,\n",
        "                relabel_nodes=True, num_nodes=N\n",
        "            )\n",
        "\n",
        "        # 패치 Data 생성\n",
        "        sub = Data(\n",
        "            x=g.x[subset] if hasattr(g, 'x') else None,\n",
        "            y=g.y[subset] if hasattr(g, 'y') else None,\n",
        "            pos=g.pos[subset] if hasattr(g, 'pos') else None,\n",
        "            edge_index=sub_ei,\n",
        "            original_nodes=subset # Store original node indices\n",
        "        )\n",
        "        if hasattr(g, 'edge_attr') and g.edge_attr is not None:\n",
        "            sub.edge_attr = g.edge_attr[edge_mask]\n",
        "\n",
        "        # 글로벌 조건 같은 부가 속성은 그대로 복사\n",
        "        if hasattr(g, 'global_cond'):\n",
        "            sub.global_cond = g.global_cond\n",
        "\n",
        "        patches.append(sub)\n",
        "        visited[subset] = True\n",
        "\n",
        "        if max_patches is not None and len(patches) >= max_patches:\n",
        "            break\n",
        "\n",
        "    return patches\n",
        "\n",
        "# === 모든 그래프에 대해 패치화 ===\n",
        "from tqdm.auto import tqdm  # (이미 상단에서 import 되어 있으면 중복 무시됨)\n",
        "\n",
        "def build_khop_patch_dataset(graphs, k=2, seeds_per_patch=1024, min_nodes=4096, max_patches_per_graph=None):\n",
        "    all_patches = []\n",
        "    # tqdm 적용\n",
        "    for gid, g in enumerate(tqdm(graphs, desc='build_khop_patches')):\n",
        "        parts = make_khop_patches(g, k=k, seeds_per_patch=seeds_per_patch,\n",
        "                                  min_nodes=min_nodes, max_patches=max_patches_per_graph)\n",
        "        for p in parts:\n",
        "            p.graph_id = gid\n",
        "        all_patches.extend(parts)\n",
        "    return all_patches"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### cluster patch method"
      ],
      "metadata": {
        "id": "IpFDLrM9L2PQ"
      },
      "id": "IpFDLrM9L2PQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Union, Iterable\n",
        "from pathlib import Path\n",
        "from itertools import chain\n",
        "from torch_geometric.data import Data\n",
        "from torch_geometric.loader import ClusterData, ClusterLoader\n",
        "\n",
        "# 단일 그래프용\n",
        "def cluster_patch_loader_one(\n",
        "    g: Data,\n",
        "    min_nodes: int = 4096,\n",
        "    save_dir: Union[str, Path, None] = None,\n",
        "    batch_size: int = 1,\n",
        "    shuffle: bool = True,\n",
        "    num_workers: int = 0,\n",
        "):\n",
        "    n_parts = max(1, g.num_nodes // min_nodes)\n",
        "    clusters = ClusterData(\n",
        "        g.cpu(),                # 전처리는 CPU에서\n",
        "        num_parts=n_parts,\n",
        "        recursive=False,\n",
        "        save_dir=save_dir       # 캐시 폴더 지정 시 파티셔닝 재사용\n",
        "    )\n",
        "    return ClusterLoader(\n",
        "        clusters,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers\n",
        "    )\n",
        "\n",
        "# 리스트/단일 모두 지원\n",
        "def cluster_patch_loader(\n",
        "    graphs: Union[Data, List[Data]],\n",
        "    min_nodes: int = 4096,\n",
        "    save_root: Union[str, Path, None] = None,  # 여러 그래프 캐시 루트\n",
        "    batch_size: int = 1,\n",
        "    shuffle: bool = True,\n",
        "    num_workers: int = 0,\n",
        ") -> Iterable:\n",
        "    if isinstance(graphs, Data):\n",
        "        return cluster_patch_loader_one(\n",
        "            graphs, min_nodes, save_root, batch_size, shuffle, num_workers\n",
        "        )\n",
        "    # 여러 그래프: 각 그래프에 대해 별도 Loader 생성 후 이어붙이기\n",
        "    loaders = []\n",
        "    for gid, g in enumerate(graphs):\n",
        "        sd = None\n",
        "        if save_root is not None:\n",
        "            sd = Path(save_root) / f\"graph_{gid}\"\n",
        "            sd.mkdir(parents=True, exist_ok=True)\n",
        "        loaders.append(\n",
        "            cluster_patch_loader_one(\n",
        "                g, min_nodes=min_nodes, save_dir=sd,\n",
        "                batch_size=batch_size, shuffle=shuffle, num_workers=num_workers\n",
        "            )\n",
        "        )\n",
        "    # chain으로 하나의 이터레이터처럼 사용\n",
        "    len_epoch = sum(len(ld) for ld in loaders)\n",
        "    return chain(*loaders), len_epoch\n"
      ],
      "metadata": {
        "id": "NK9ryPlYHsNp"
      },
      "id": "NK9ryPlYHsNp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear CUDA cache\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(\"CUDA cache cleared.\")\n",
        "\n",
        "# Collect garbage\n",
        "gc.collect()\n",
        "print(\"Garbage collected.\")6\n",
        "\n",
        "0.\n",
        "\n",
        "\n",
        "\n",
        "# Windows면 num_workers=0 권장, 리눅스/코랩은 2~8 시도\n",
        "train_loader, len_epoch = cluster_patch_loader(\n",
        "    train_graphs, min_nodes=4096,\n",
        "    save_root=\"cluster_cache2/train\",  # 옵션: 파티션 캐시\n",
        "    batch_size=4, shuffle=True, num_workers=4\n",
        ")\n",
        "val_loader, len_val = cluster_patch_loader(\n",
        "    val_graphs, min_nodes=4096,\n",
        "    save_root=\"cluster_cache2/val\",\n",
        "    batch_size=4, shuffle=False, num_workers=4\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "G8M2Ph2hVUJE",
        "outputId": "fcb75189-1957-45ca-8cc6-168478fd5a13"
      },
      "id": "G8M2Ph2hVUJE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA cache cleared.\n",
            "Garbage collected.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n",
            "Computing METIS partitioning...\n",
            "Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utilities"
      ],
      "metadata": {
        "id": "W-SMGIA4NvMi"
      },
      "id": "W-SMGIA4NvMi"
    },
    {
      "cell_type": "code",
      "source": [
        "# === dims 자동추론 유틸 ===\n",
        "def infer_dims_from_loader(loader):\n",
        "    # 첫 배치 하나만 뽑기 (loader는 이터레이터라 소모됨)\n",
        "    sample = next(iter(loader))\n",
        "    # x: [N, F]\n",
        "    if not hasattr(sample, 'x') or sample.x is None:\n",
        "        raise ValueError(\"sample.x가 없습니다. in_dim을 알 수 없어요.\")\n",
        "    in_dim = sample.x.shape[1]\n",
        "\n",
        "    # y: [N] 또는 [N, C]\n",
        "    if not hasattr(sample, 'y') or sample.y is None:\n",
        "        raise ValueError(\"sample.y가 없습니다. target_dim을 알 수 없어요.\")\n",
        "    y = sample.y\n",
        "    target_dim = y.shape[1] if y.dim() == 2 else 1\n",
        "\n",
        "    # global_cond(조건 벡터)가 있다면 차원 자동 감지\n",
        "    cond_dim = 0\n",
        "    if hasattr(sample, 'global_cond') and sample.global_cond is not None:\n",
        "        gc_ = sample.global_cond\n",
        "        # 배치 단위로 브로드캐스트된 경우 [N, D]일 수도, 그래프별이면 [B, D] 혹은 [D]\n",
        "        if gc_.dim() == 1:\n",
        "            cond_dim = gc_.shape[0]\n",
        "        else:\n",
        "            cond_dim = gc_.shape[-1]\n",
        "\n",
        "    return in_dim, target_dim, cond_dim\n"
      ],
      "metadata": {
        "id": "lahKLixANx1a"
      },
      "id": "lahKLixANx1a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop"
      ],
      "metadata": {
        "id": "uphztnQ7VX13"
      },
      "id": "uphztnQ7VX13"
    },
    {
      "cell_type": "code",
      "source": [
        "in_dim, target_dim, inferred_cond_dim = infer_dims_from_loader(train_loader)\n"
      ],
      "metadata": {
        "id": "_FdAkKBVNy8M"
      },
      "id": "_FdAkKBVNy8M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(in_dim, target_dim, inferred_cond_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uc68olzN26W",
        "outputId": "66064b9b-553b-4278-86b6-b63a488993ad"
      },
      "id": "1uc68olzN26W",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8 1 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42d9f955",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "id": "42d9f955",
        "outputId": "37d5b744-f0d3-4376-ad08-448fec658023"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.21.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250819_074832-tuisu8ci</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jyk-snuai-seoul-national-university/gnn-pinn-training/runs/tuisu8ci' target=\"_blank\">surface_only_1755589712</a></strong> to <a href='https://wandb.ai/jyk-snuai-seoul-national-university/gnn-pinn-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jyk-snuai-seoul-national-university/gnn-pinn-training' target=\"_blank\">https://wandb.ai/jyk-snuai-seoul-national-university/gnn-pinn-training</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jyk-snuai-seoul-national-university/gnn-pinn-training/runs/tuisu8ci' target=\"_blank\">https://wandb.ai/jyk-snuai-seoul-national-university/gnn-pinn-training/runs/tuisu8ci</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[001] train 0.043517 | val 0.010127 | lr 9.78e-05\n",
            "[005] train 0.000000 | val 0.000000 | lr 9.78e-05\n",
            "[010] train 0.000000 | val 0.000000 | lr 9.78e-05\n",
            "[015] train 0.000000 | val 0.000000 | lr 9.78e-05\n",
            "[020] train 0.000000 | val 0.000000 | lr 9.78e-05\n",
            "[025] train 0.000000 | val 0.000000 | lr 9.78e-05\n",
            "[030] train 0.000000 | val 0.000000 | lr 9.78e-05\n",
            "[035] train 0.000000 | val 0.000000 | lr 9.78e-05\n",
            "[040] train 0.000000 | val 0.000000 | lr 9.78e-05\n",
            "[045] train 0.000000 | val 0.000000 | lr 9.78e-05\n",
            "[050] train 0.000000 | val 0.000000 | lr 9.78e-05\n",
            "[055] train 0.000000 | val 0.000000 | lr 9.78e-05\n",
            "[060] train 0.000000 | val 0.000000 | lr 9.78e-05\n",
            "[065] train 0.000000 | val 0.000000 | lr 9.78e-05\n",
            "[070] train 0.000000 | val 0.000000 | lr 9.78e-05\n",
            "[075] train 0.000000 | val 0.000000 | lr 9.78e-05\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch_geometric.data import Batch\n",
        "from torch_geometric.loader import DataLoader, NeighborLoader\n",
        "from torch import Tensor\n",
        "import gc\n",
        "import wandb\n",
        "import time\n",
        "\n",
        "\n",
        "# --------- 하이퍼파라미터 ---------\n",
        "BATCH_SIZE   = 1\n",
        "LR           = 1e-4\n",
        "WEIGHT_DECAY = 1e-5\n",
        "EPOCHS       = 75\n",
        "PRINT_EVERY  = 5\n",
        "GRAD_CLIP    = 5.0\n",
        "\n",
        "# surface-only loss 가중치\n",
        "LOSS_WEIGHTS = {\"data\": 1.0, \"tv\": 0.05, \"lap\": 0.01}\n",
        "USE_TV = True\n",
        "EPS = 1e-12\n",
        "\n",
        "# --------- device ----------\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)\n",
        "\n",
        "\n",
        "# --------- 모델 정의/이동 ----------\n",
        "# ⚠️ out_dim을 타겟 채널 수로!\n",
        "\n",
        "model = BoundaryGraphNet(\n",
        "    in_dim=in_dim,\n",
        "    hidden=256,\n",
        "    layers=4,\n",
        "    dropout=0.1,\n",
        "    heads=2,\n",
        "    use_pos_enc=True,\n",
        "    pos_freqs=2,\n",
        "    cond_dim=4   # 예: [Uinf, rho, nu, A_ref]를 data.global_cond로 넣을 때\n",
        ").to(device)\n",
        "\n",
        "# --------- optimizer (+옵션: 스케줄러) ----------\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
        "scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2, eta_min=LR*0.1)\n",
        "\n",
        "# --------- W&B init ----------\n",
        "os.environ[\"WANDB_DISABLE_SYMLINKS\"] = \"true\"  # <- wandb.init 이전에!\n",
        "\n",
        "wandb_run = wandb.init(\n",
        "    project=\"gnn-pinn-training\",     # <- 프로젝트명 바꿔도 됨\n",
        "    name=f\"surface_only_{int(time.time())}\",\n",
        "    config={\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"lr\": LR,\n",
        "        \"weight_decay\": WEIGHT_DECAY,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"grad_clip\": GRAD_CLIP,\n",
        "        \"loss_weights\": LOSS_WEIGHTS,\n",
        "        \"use_tv\": USE_TV,\n",
        "        \"feat_dim\": in_dim,\n",
        "        \"target_dim\": target_dim,\n",
        "        \"model\": model.__class__.__name__,\n",
        "        \"optimizer\": \"Adam\",\n",
        "    },\n",
        ")\n",
        "# 과도한 비용 피하려면 watch는 주석처리 가능\n",
        "# wandb.watch(model, log=\"all\", log_freq=50)\n",
        "\n",
        "# --------- 기하 도우미/손실 ---------\n",
        "def edge_geo_terms(pos: Tensor, edge_index: Tensor):\n",
        "    i, j = edge_index\n",
        "    e_ij   = pos[j] - pos[i]\n",
        "    len_ij = e_ij.norm(dim=1, keepdim=True).clamp_min(torch.finfo(pos.dtype).tiny)\n",
        "    t_ij   = e_ij / len_ij\n",
        "    return i, j, t_ij, len_ij\n",
        "\n",
        "def graph_edge_weights(pos: Tensor, edge_index: Tensor, mode=\"invlen\", clamp=10.0):\n",
        "    _, _, _, len_ij = edge_geo_terms(pos, edge_index)\n",
        "    if mode == \"invlen\":\n",
        "        w = (1.0 / (len_ij + EPS)).squeeze(1)\n",
        "        if clamp is not None:\n",
        "            w = w.clamp_max(clamp)\n",
        "        return w\n",
        "    else:\n",
        "        return torch.ones(edge_index.size(1), device=pos.device, dtype=pos.dtype)\n",
        "\n",
        "def edge_tv_or_l2(y_pred: Tensor, edge_index: Tensor, edge_w: Tensor, use_tv=True):\n",
        "    i, j = edge_index\n",
        "    diff = y_pred[i] - y_pred[j]  # (E,C)\n",
        "    if use_tv:\n",
        "        return (diff.abs() * edge_w.unsqueeze(-1)).mean()\n",
        "    else:\n",
        "        return ((diff**2) * edge_w.unsqueeze(-1)).mean()\n",
        "\n",
        "def laplacian_reg(y_pred, edge_index, num_nodes, edge_w=None, eps=1e-12):\n",
        "    \"\"\"\n",
        "    y_pred: [N, C]\n",
        "    edge_index: [2, E] (long)\n",
        "    edge_w: [E] or None\n",
        "    \"\"\"\n",
        "    # --- 준비: dtype/device 정렬 ---\n",
        "    device = y_pred.device\n",
        "    dtype  = torch.float32  # 라플라시안은 fp32로 계산 권장\n",
        "    # (원래 y_pred가 fp16/ bf16여도 여기선 fp32로 올려 계산 후 다시 캐스팅)\n",
        "\n",
        "    # 인덱스는 long, 같은 디바이스\n",
        "    i, j = edge_index\n",
        "    i = i.to(device=device, dtype=torch.long)\n",
        "    j = j.to(device=device, dtype=torch.long)\n",
        "\n",
        "    E = i.numel()\n",
        "    C = y_pred.size(1)\n",
        "\n",
        "    # 엣지 가중치\n",
        "    if edge_w is None:\n",
        "        edge_w = torch.ones(E, device=device, dtype=dtype)\n",
        "    else:\n",
        "        edge_w = edge_w.to(device=device, dtype=dtype)\n",
        "\n",
        "    # y를 fp32로\n",
        "    y = y_pred.to(dtype)\n",
        "\n",
        "    # --- degree 및 가중합 ---\n",
        "    deg = torch.zeros(num_nodes, device=device, dtype=dtype)\n",
        "    deg.scatter_add_(0, i, edge_w)  # deg[u] = sum_v w_uv\n",
        "\n",
        "    wyj = torch.zeros((num_nodes, C), device=device, dtype=dtype)\n",
        "    wyj.scatter_add_(0, i.unsqueeze(-1).expand(-1, C),\n",
        "                     (edge_w.unsqueeze(-1) * y[j]))\n",
        "\n",
        "    # 라플라시안 L y = D y - W y  (여기선 행 정규화/대칭 정규화 없이 단순형)\n",
        "    Ly = deg.unsqueeze(-1) * y - wyj\n",
        "\n",
        "    # 규제값: ||Ly||^2 (노드/채널 평균)\n",
        "    reg = (Ly.pow(2).sum(dim=1)).mean()\n",
        "\n",
        "    # 원래 dtype으로 캐스팅해서 리턴\n",
        "    return reg.to(y_pred.dtype)\n",
        "\n",
        "\n",
        "def surface_only_loss(batch, pred: Tensor, loss_weights=LOSS_WEIGHTS):\n",
        "    x = batch.x\n",
        "    y = batch.y\n",
        "    edge_index = batch.edge_index\n",
        "    pos = batch.pos if hasattr(batch, 'pos') and batch.pos is not None else x[:, :3]\n",
        "\n",
        "    data_loss = nn.functional.mse_loss(pred, y)\n",
        "    w_e = graph_edge_weights(pos, edge_index, mode=\"invlen\")\n",
        "\n",
        "    tv_loss  = edge_tv_or_l2(pred, edge_index, w_e, use_tv=USE_TV)\n",
        "\n",
        "    if hasattr(batch, 'batch') and batch.batch is not None:\n",
        "        lap_loss_acc = 0.0\n",
        "        uniq = batch.batch.unique()\n",
        "        for g_id in uniq:\n",
        "            mask = (batch.batch == g_id)\n",
        "            node_idx = torch.nonzero(mask, as_tuple=False).squeeze(1)\n",
        "            mask_i = mask[edge_index[0]]\n",
        "            mask_j = mask[edge_index[1]]\n",
        "            e_mask = mask_i & mask_j\n",
        "            if e_mask.sum() == 0:\n",
        "                continue\n",
        "            sub_e = edge_index[:, e_mask]\n",
        "            old2new = -torch.ones(mask.size(0), device=mask.device, dtype=torch.long)\n",
        "            old2new[node_idx] = torch.arange(node_idx.size(0), device=mask.device)\n",
        "            sub_e = old2new[sub_e]\n",
        "            lap_loss_acc = lap_loss_acc + laplacian_reg(pred[mask], sub_e, node_idx.size(0), w_e[e_mask])\n",
        "        lap_loss = lap_loss_acc / (uniq.numel() + EPS)\n",
        "    else:\n",
        "        lap_loss = laplacian_reg(pred, edge_index, x.size(0), w_e)\n",
        "\n",
        "    loss = (loss_weights[\"data\"] * data_loss +\n",
        "            loss_weights[\"tv\"]   * tv_loss  +\n",
        "            loss_weights[\"lap\"]  * lap_loss)\n",
        "\n",
        "    return loss, {\"loss_data\": data_loss.detach(),\n",
        "                  \"loss_tv\":   tv_loss.detach(),\n",
        "                  \"loss_lap\":  lap_loss.detach()}\n",
        "\n",
        "# ======================\n",
        "# Training / Validation\n",
        "# ======================\n",
        "best_val = float('inf')\n",
        "best_path = \"best_surface_only.pt\"\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    model.train()\n",
        "    tr_loss_sum = 0.0; tr_nodes = 0\n",
        "    ep_data = ep_tv = ep_lap = 0.0\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=(device.type=='cuda'))\n",
        "\n",
        "    for step, batch in enumerate(train_loader, start=1):\n",
        "        batch = batch.to(device, non_blocking=True)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with torch.cuda.amp.autocast(enabled=(device.type=='cuda')):\n",
        "            pred = model(batch)\n",
        "            if pred.dim() == 1:\n",
        "                pred = pred.unsqueeze(1)\n",
        "            loss, parts = surface_only_loss(batch, pred)\n",
        "\n",
        "        # ★ 올바른 순서: backward -> unscale -> clip -> step -> update\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.unscale_(optimizer)\n",
        "        if GRAD_CLIP and GRAD_CLIP > 0:\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        # ★ CosineAnnealingWarmRestarts는 iteration 단위 step이 자연스러움\n",
        "        scheduler.step(epoch - 1 + step / len_epoch)\n",
        "\n",
        "        n = batch.x.size(0)\n",
        "        tr_loss_sum += loss.item() * n\n",
        "        tr_nodes    += n\n",
        "        ep_data += parts[\"loss_data\"].item() * n\n",
        "        ep_tv   += parts[\"loss_tv\"].item()   * n\n",
        "        ep_lap  += parts[\"loss_lap\"].item()  * n\n",
        "\n",
        "    train_loss = tr_loss_sum / max(tr_nodes, 1)\n",
        "    train_data = ep_data / max(tr_nodes, 1)\n",
        "    train_tv   = ep_tv   / max(tr_nodes, 1)\n",
        "    train_lap  = ep_lap  / max(tr_nodes, 1)\n",
        "\n",
        "    # -------- Validation --------\n",
        "    model.eval()\n",
        "    va_loss_sum = 0.0\n",
        "    va_nodes    = 0\n",
        "    va_data, va_tv, va_lap = 0.0, 0.0, 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = batch.to(device, non_blocking=True)\n",
        "            pred = model(batch)\n",
        "            if pred.dim() == 1:\n",
        "                pred = pred.unsqueeze(1)\n",
        "            v_loss, v_parts = surface_only_loss(batch, pred)\n",
        "\n",
        "            n = batch.x.size(0)\n",
        "            data_loss = v_parts[\"loss_data\"]           # 데이터 MSE만 사용\n",
        "            va_loss_sum += data_loss.item() * n   # total 대신 data loss만 누적\n",
        "            va_nodes    += n\n",
        "\n",
        "            va_data += v_parts[\"loss_data\"].item() * n\n",
        "            va_tv   += v_parts[\"loss_tv\"].item()   * n\n",
        "            va_lap  += v_parts[\"loss_lap\"].item()  * n\n",
        "\n",
        "    val_loss = va_loss_sum / max(va_nodes, 1)\n",
        "    val_data = va_data / max(va_nodes, 1)\n",
        "    val_tv   = va_tv   / max(va_nodes, 1)\n",
        "    val_lap  = va_lap  / max(va_nodes, 1)\n",
        "\n",
        "    # -------- W&B logging --------\n",
        "    current_lr = optimizer.param_groups[0][\"lr\"]\n",
        "    wandb.log({\n",
        "        \"epoch\": epoch,\n",
        "        \"lr\": current_lr,\n",
        "        \"train/loss\": train_loss,\n",
        "        \"train/loss_data\": train_data,\n",
        "        \"train/loss_tv\": train_tv,\n",
        "        \"train/loss_lap\": train_lap,\n",
        "        \"val/loss\": val_loss,\n",
        "        \"val/loss_data\": val_data,\n",
        "        \"val/loss_tv\": val_tv,\n",
        "        \"val/loss_lap\": val_lap,\n",
        "    })\n",
        "\n",
        "    if epoch % PRINT_EVERY == 0 or epoch == 1:\n",
        "        print(f\"[{epoch:03d}] train {train_loss:.6f} | val {val_loss:.6f} | lr {current_lr:.2e}\")\n",
        "\n",
        "    # -------- best model 저장 & W&B 업로드 --------\n",
        "    if val_loss < best_val:\n",
        "        best_val = val_loss\n",
        "        torch.save({\"model\": model.state_dict(),\n",
        "                    \"optimizer\": optimizer.state_dict(),\n",
        "                    \"epoch\": epoch,\n",
        "                    \"val_loss\": best_val,\n",
        "                    \"config\": wandb.config}, best_path)\n",
        "        # 파일을 W&B에 첨부 (Artifacts가 필요 없으면 이걸로 충분)\n",
        "        artifact = wandb.Artifact(\n",
        "        name=\"best_model\",\n",
        "        type=\"model\",\n",
        "        metadata={\"val_loss\": float(best_val)}\n",
        "        )\n",
        "        artifact.add_file(best_path, name=\"best_surface_only.pt\")\n",
        "        wandb.log_artifact(artifact)  # <- 이 한 줄이면 끝\n",
        "\n",
        "# run 종료\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation by visualization"
      ],
      "metadata": {
        "id": "qoMteHUsVbtw"
      },
      "id": "qoMteHUsVbtw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a5e59bb",
      "metadata": {
        "id": "9a5e59bb"
      },
      "outputs": [],
      "source": [
        "import pyvista as pv\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# 고정 컬러범위(자동으로 쓰고 싶으면 vmin=vmax=None)\n",
        "vmin, vmax = -0.5, 0.0\n",
        "\n",
        "def _to_numpy(x):\n",
        "    if torch.is_tensor(x):\n",
        "        return x.detach().cpu().numpy()\n",
        "    return np.asarray(x)\n",
        "\n",
        "def _collect_per_node_scalars(sample, device, N_expected):\n",
        "    \"\"\"\n",
        "    sample 안에서 길이 N_expected 인 1D 스칼라(또는 (N,1))들을 찾아 dict로 반환.\n",
        "    y가 (N,k)면 각 열을 y[:,i]로 분해해서 y_col_i 로 넣음.\n",
        "    \"\"\"\n",
        "    cands = {}\n",
        "\n",
        "    # x, y 우선\n",
        "    if hasattr(sample, 'y') and sample.y is not None:\n",
        "        y = sample.y.to(device)\n",
        "        if y.dim() == 1 and y.shape[0] == N_expected:\n",
        "            cands['y'] = _to_numpy(y)\n",
        "        elif y.dim() == 2 and y.shape[0] == N_expected:\n",
        "            for i in range(y.shape[1]):\n",
        "                arr = y[:, i]\n",
        "                if arr.dim() == 1:\n",
        "                    cands[f'y_col_{i}'] = _to_numpy(arr)\n",
        "                elif arr.dim() == 2 and arr.shape[1] == 1:\n",
        "                    cands[f'y_col_{i}'] = _to_numpy(arr[:, 0])\n",
        "\n",
        "    # 일반 속성들 순회\n",
        "    for name, val in vars(sample).items():\n",
        "        # 이미 처리한 y/x 제외\n",
        "        if name in ('y', 'x'):\n",
        "            continue\n",
        "        try:\n",
        "            arr = val.to(device) if torch.is_tensor(val) else val\n",
        "        except Exception:\n",
        "            continue\n",
        "\n",
        "        if torch.is_tensor(arr) or isinstance(arr, np.ndarray):\n",
        "            arr_np = _to_numpy(arr)\n",
        "            if arr_np.ndim == 1 and arr_np.shape[0] == N_expected:\n",
        "                cands[name] = arr_np\n",
        "            elif arr_np.ndim == 2 and arr_np.shape[0] == N_expected and arr_np.shape[1] == 1:\n",
        "                cands[name] = arr_np[:, 0]\n",
        "\n",
        "        # dict 스타일(node_data 등)\n",
        "        if isinstance(val, dict):\n",
        "            for k, v in val.items():\n",
        "                arr2 = v.to(device) if torch.is_tensor(v) else v\n",
        "                if torch.is_tensor(arr2) or isinstance(arr2, np.ndarray):\n",
        "                    arr2_np = _to_numpy(arr2)\n",
        "                    if arr2_np.ndim == 1 and arr2_np.shape[0] == N_expected:\n",
        "                        cands[f'{name}.{k}'] = arr2_np\n",
        "                    elif arr2_np.ndim == 2 and arr2_np.shape[0] == N_expected and arr2_np.shape[1] == 1:\n",
        "                        cands[f'{name}.{k}'] = arr2_np[:, 0]\n",
        "\n",
        "    return cands\n",
        "\n",
        "def _select_gt_pressure(cands, p_pred):\n",
        "    \"\"\"\n",
        "    이름 매칭(press|pressure|^p$) 우선,\n",
        "    없으면 |corr| 최대 후보 선택.\n",
        "    \"\"\"\n",
        "    if not cands:\n",
        "        raise RuntimeError(\"샘플에서 노드별 스칼라 후보를 찾지 못했습니다.\")\n",
        "\n",
        "    # 1) 이름 우선 매칭\n",
        "    pat = re.compile(r'(?:^|[_.-])(p|press|pressure)(?:$|[_.-])', re.IGNORECASE)\n",
        "    name_matches = [k for k in cands.keys() if pat.search(k)]\n",
        "    if len(name_matches) == 1:\n",
        "        k = name_matches[0]\n",
        "        print(f\"GT pressure chosen by name: '{k}'\")\n",
        "        return cands[k], k\n",
        "    elif len(name_matches) > 1:\n",
        "        # 이름 후보가 여러 개라면, corr로 좁힘\n",
        "        name_matches = name_matches\n",
        "\n",
        "    # 2) 상관계수 최대(|r|)\n",
        "    keys = name_matches if name_matches else list(cands.keys())\n",
        "    def _corr(a, b):\n",
        "        a = np.asarray(a).reshape(-1)\n",
        "        b = np.asarray(b).reshape(-1)\n",
        "        if a.std() < 1e-12 or b.std() < 1e-12:\n",
        "            return 0.0\n",
        "        return float(np.corrcoef(a, b)[0, 1])\n",
        "\n",
        "    scores = [(k, _corr(cands[k], p_pred)) for k in keys]\n",
        "    scores_sorted = sorted(scores, key=lambda x: abs(x[1]), reverse=True)\n",
        "    best_k, best_r = scores_sorted[0]\n",
        "    print(\"Candidate correlations with p_pred (top 5):\")\n",
        "    for k, r in scores_sorted[:5]:\n",
        "        print(f\"  {k:>24s} : r = {r:+.4f}\")\n",
        "    print(f\"GT pressure chosen by correlation: '{best_k}' (|r|={abs(best_r):.4f})\")\n",
        "    return cands[best_k], best_k\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_geometric.data import Batch\n",
        "\n",
        "# Concatenate val_patches[1] through val_patches[20] (inclusive) into a single batch\n",
        "start_idx, end_idx = 1, 200  # inclusive range\n",
        "assert len(val_patches) > end_idx, f\"Need at least {end_idx+1} validation patches, got {len(val_patches)}\"\n",
        "subset = val_patches[start_idx:end_idx+1]\n",
        "val_concat = Batch.from_data_list(subset).to(device)\n",
        "\n",
        "print(f\"Concatenated patches {start_idx}~{end_idx}\")\n",
        "print(f\"Total nodes: {val_concat.num_nodes}\")\n",
        "print(f\"Total edges: {val_concat.edge_index.size(1)}\")\n",
        "print(f\"Batch vector size: {val_concat.batch.size()} | num graphs: {val_concat.num_graphs}\")\n",
        "\n",
        "# Optional: move to device if needed\n",
        "# val_concat = val_concat.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A03dLDZNT_wE",
        "outputId": "efbc1969-2184-4e98-b34b-9efff26235f8"
      },
      "id": "A03dLDZNT_wE",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Concatenated patches 2~200\n",
            "Total nodes: 591057\n",
            "Total edges: 3369650\n",
            "Batch vector size: torch.Size([591057]) | num graphs: 199\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68c19e7a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68c19e7a",
        "outputId": "5de143c8-a89d-4053-ed83-963afe3349e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting batched inference on validation patches...\n",
            "Batched inference time: 2.146s\n",
            "Visualizing results for the first validation graph (ID 0) with 429 patches...\n",
            "[Full Graph 0] MAE: 0.150647, RMSE: 0.227161\n",
            "Saved side-by-side comparison to: pred_vs_true_pressure_graph0.png\n"
          ]
        }
      ],
      "source": [
        "import pyvista as pv\n",
        "import numpy as np\n",
        "import torch\n",
        "import re\n",
        "import time\n",
        "from torch_geometric.data import Batch\n",
        "\n",
        "# Use existing globals if set, otherwise compute defaults later\n",
        "vmin = globals().get('vmin', None)\n",
        "vmax = globals().get('vmax', None)\n",
        "\n",
        "# Helper to convert tensors/arrays to numpy\n",
        "def _to_numpy(x):\n",
        "    if torch.is_tensor(x):\n",
        "        return x.detach().cpu().numpy()\n",
        "    return np.asarray(x)\n",
        "\n",
        "\n",
        "# --- MODIFIED INFERENCE CODE ---\n",
        "\n",
        "# Assuming val_concat was created from a list of Data objects (patches)\n",
        "# We need to perform inference on these patches individually or in batches.\n",
        "# Reuse the val_loader created earlier, which already handles batching.\n",
        "\n",
        "all_preds = []\n",
        "all_nodes = []\n",
        "\n",
        "print(\"Starting batched inference on validation patches...\")\n",
        "start = time.time()\n",
        "\n",
        "# Ensure model is in eval mode and on the correct device\n",
        "model.eval()\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device) # Ensure model is on device before inference\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, batch in enumerate(val_loader):\n",
        "        batch = batch.to(device, non_blocking=True)\n",
        "\n",
        "        # Perform inference\n",
        "        out = model(batch)\n",
        "        if isinstance(out, tuple):\n",
        "            pred = out[0]\n",
        "        else:\n",
        "            pred = out\n",
        "\n",
        "        if pred.dim() == 1:\n",
        "            pred = pred.unsqueeze(1)\n",
        "\n",
        "        # Collect predictions and original node indices (if available in batch)\n",
        "        # If batch comes from DataLoader of patches, original node indices might not be preserved\n",
        "        # However, for visualization, we need to map predictions back to original mesh positions.\n",
        "        # For this example, let's assume the patches cover the relevant part of the original mesh\n",
        "        # and we can concatenate predictions directly.\n",
        "        # If patches overlap or represent subsets needing remapping, a more complex\n",
        "        # aggregation/remapping logic would be needed here.\n",
        "\n",
        "        # For simplicity and visualization, concatenate predictions directly.\n",
        "        # This assumes the order of nodes in concatenated patches matches some order in original mesh\n",
        "        # or that the visualization only needs relative positions within batches.\n",
        "        # A better approach would involve storing and using original node indices from patches.\n",
        "        all_preds.append(pred.detach().cpu()) # Move back to CPU to avoid memory issues during concatenation\n",
        "\n",
        "        # For visualization, we need the original coordinates corresponding to these predictions.\n",
        "        # Assuming batch.pos contains the positions relative to each patch or original positions\n",
        "        # We collect positions along with predictions.\n",
        "        all_nodes.append(batch.pos.detach().cpu()) # Collect positions\n",
        "\n",
        "elapsed = time.time() - start\n",
        "print(f'Batched inference time: {elapsed:.3f}s')\n",
        "\n",
        "# Concatenate predictions and positions from all batches\n",
        "p_pred = torch.cat(all_preds, dim=0).numpy()[:, 0]\n",
        "coords = torch.cat(all_nodes, dim=0).numpy()\n",
        "N = coords.shape[0] # Total number of nodes across all batches\n",
        "\n",
        "# --- END MODIFIED INFERENCE CODE ---\n",
        "\n",
        "\n",
        "# Assuming 'sample' here refers conceptually to the full validation graph\n",
        "# from which val_patches were generated, for getting ground truth.\n",
        "# If you need GT for each batch, you'd access batch.y inside the loop.\n",
        "# For visualization of the *full* validation set prediction, we need the\n",
        "# GT for the corresponding nodes.\n",
        "\n",
        "# To get GT for visualization of the full val set, we need the original val_graphs\n",
        "# and find the nodes that correspond to the collected coords.\n",
        "# This requires a mapping which is not directly available from the DataLoader batch.\n",
        "# A simpler approach for visualization is to plot one of the original val_graphs\n",
        "# and its prediction obtained by running inference on its corresponding patches.\n",
        "\n",
        "# Let's visualize the prediction and ground truth for the *first* graph in the validation set\n",
        "# by running inference on its patches and then reassembling.\n",
        "\n",
        "# --- Visualization for the first validation graph ---\n",
        "if val_graphs:\n",
        "    first_val_graph = val_graphs[0]\n",
        "    # Find patches belonging to the first graph\n",
        "    first_graph_patches = [p for p in val_patches if hasattr(p, 'graph_id') and p.graph_id == 0]\n",
        "\n",
        "    if first_graph_patches:\n",
        "        print(f\"Visualizing results for the first validation graph (ID 0) with {len(first_graph_patches)} patches...\")\n",
        "\n",
        "        # Create a DataLoader for these specific patches\n",
        "        first_graph_loader = DataLoader(\n",
        "            first_graph_patches, batch_size=BATCH_SIZE, shuffle=False,\n",
        "            pin_memory=pin_memory, num_workers=0, persistent_workers=False\n",
        "        )\n",
        "\n",
        "        patch_preds = []\n",
        "        patch_nodes_pos = []\n",
        "        patch_nodes_original_indices = [] # We need original indices to map predictions back to the full graph\n",
        "\n",
        "        # Re-process patches to include original node indices if not already present\n",
        "        # (Assuming original nodes are 0 to N-1 in the source graph)\n",
        "        # If your patch generation already stores original indices, use them.\n",
        "        # Otherwise, you might need to modify make_khop_patches or build_khop_patch_dataset\n",
        "        # to store the 'subset' tensor (original indices) in each patch Data object.\n",
        "\n",
        "        # For demonstration, let's assume patch.original_nodes contains the indices in the *original* graph\n",
        "        # If not, you would need to modify patch creation or find another way to map.\n",
        "        # Example modification during patch creation: p.original_nodes = subset\n",
        "\n",
        "        # Assuming patches have an 'original_nodes' attribute containing indices from the full graph\n",
        "        # If not, this part needs adjustment based on how patches were created.\n",
        "        if not hasattr(first_graph_patches[0], 'original_nodes'):\n",
        "             print(\"WARNING: Patches do not have 'original_nodes' attribute. Visualization mapping might be incorrect.\")\n",
        "             print(\"Consider adding 'p.original_nodes = subset' in make_khop_patches.\")\n",
        "             # Fallback: Use concatenated positions, assuming they are ordered correctly\n",
        "             # This is less reliable for overlapping patches.\n",
        "             all_patched_pos = torch.cat([p.pos.detach().cpu() for p in first_graph_patches], dim=0).numpy()\n",
        "             # Create a dummy mapping based on concatenation order - USE WITH CAUTION\n",
        "             unique_pos, inverse_indices = np.unique(all_patched_pos, axis=0, return_inverse=True)\n",
        "             # This fallback is likely incorrect for complex patching schemes.\n",
        "             # A robust solution requires storing original node indices in patches.\n",
        "\n",
        "        collected_preds = torch.zeros(first_val_graph.num_nodes, pred.size(1), device='cpu')\n",
        "        # Assuming average prediction for overlapping nodes\n",
        "        node_counts = torch.zeros(first_val_graph.num_nodes, 1, device='cpu')\n",
        "\n",
        "\n",
        "        model.eval() # Just in case\n",
        "        with torch.no_grad():\n",
        "            for i, batch in enumerate(first_graph_loader):\n",
        "                 batch = batch.to(device, non_blocking=True)\n",
        "                 out = model(batch)\n",
        "                 if isinstance(out, tuple):\n",
        "                    pred = out[0]\n",
        "                 else:\n",
        "                    pred = out\n",
        "                 if pred.dim() == 1:\n",
        "                    pred = pred.unsqueeze(1)\n",
        "\n",
        "                 # Map predictions back to original graph indices\n",
        "                 # This requires the original indices of nodes within the batch\n",
        "                 # Assuming `batch.original_nodes` exists and holds the original indices\n",
        "                 if hasattr(batch, 'original_nodes'):\n",
        "                     original_indices = batch.original_nodes.cpu()\n",
        "                     collected_preds[original_indices] += pred.detach().cpu()\n",
        "                     node_counts[original_indices] += 1\n",
        "                 else:\n",
        "                     print(f\"Warning: Batch {i} missing 'original_nodes'. Cannot map predictions to original graph for visualization.\")\n",
        "                     # If no mapping, cannot accurately visualize on the full graph structure.\n",
        "                     # You might need to adjust `make_khop_patches` to store `subset`.\n",
        "                     # For now, break if mapping is impossible.\n",
        "                     collected_preds = None # Indicate mapping failed\n",
        "                     break\n",
        "\n",
        "        if collected_preds is not None:\n",
        "            # Average predictions for overlapping nodes\n",
        "            collected_preds = collected_preds / (node_counts + 1e-8)\n",
        "            p_pred_full_graph = collected_preds.numpy()[:, 0]\n",
        "            coords_full_graph = first_val_graph.pos.detach().cpu().numpy()\n",
        "            p_true_full_graph = first_val_graph.y.detach().cpu().numpy()[:, 0] # Assuming y[:,0] is pressure\n",
        "\n",
        "            # Calculate metrics on the full graph prediction\n",
        "            mae_full = float(np.mean(np.abs(p_pred_full_graph - p_true_full_graph)))\n",
        "            rmse_full = float(np.sqrt(np.mean((p_pred_full_graph - p_true_full_graph) ** 2)))\n",
        "            print(f\"[Full Graph 0] MAE: {mae_full:.6f}, RMSE: {rmse_full:.6f}\")\n",
        "\n",
        "            # Determine color limits for visualization\n",
        "            if vmin is None or vmax is None:\n",
        "                _min = float(min(p_pred_full_graph.min(), p_true_full_graph.min()))\n",
        "                _max = float(max(p_pred_full_graph.max(), p_true_full_graph.max()))\n",
        "                pad = 0.05 * (_max - _min + 1e-8)\n",
        "                vmin_, vmax_ = _min - pad, _max + pad\n",
        "            else:\n",
        "                vmin_, vmax_ = vmin, vmax\n",
        "\n",
        "\n",
        "            # PV data for the full graph\n",
        "            cloud_true_full = pv.PolyData(coords_full_graph.copy()); cloud_true_full['pressure_true'] = p_true_full_graph\n",
        "            cloud_pred_full = pv.PolyData(coords_full_graph.copy()); cloud_pred_full['pressure_pred'] = p_pred_full_graph\n",
        "\n",
        "            # 1x2 plot + save\n",
        "            pl = pv.Plotter(shape=(1, 2), off_screen=True, border=True)\n",
        "            pl.subplot(0, 0)\n",
        "            pl.add_text(f\"Ground Truth (p) [Graph 0]\", font_size=12)\n",
        "            pl.add_mesh(cloud_true_full, scalars='pressure_true', cmap='viridis', clim=(vmin_, vmax_), point_size=6)\n",
        "\n",
        "            pl.subplot(0, 1)\n",
        "            pl.add_text(\"Prediction (p) [Graph 0]\", font_size=12)\n",
        "            pl.add_mesh(cloud_pred_full, scalars='pressure_pred', cmap='viridis', clim=(vmin_, vmax_), point_size=6)\n",
        "\n",
        "            pl.link_views()\n",
        "            pl.view_isometric()\n",
        "            out_png = f\"pred_vs_true_pressure_graph0.png\"\n",
        "            pl.show(screenshot=out_png)\n",
        "            print(f\"Saved side-by-side comparison to: {out_png}\")\n",
        "\n",
        "            # (Optional) Save for ParaView\n",
        "            # cloud_true_full.save(f\"pressure_true_graph0.vtp\")\n",
        "            # cloud_pred_full.save(f\"pressure_pred_graph0.vtp\")\n",
        "\n",
        "        else:\n",
        "            print(\"Skipping visualization for Graph 0 due to missing original node mapping.\")\n",
        "\n",
        "    else:\n",
        "        print(\"No patches found for the first validation graph (ID 0). Skipping visualization.\")\n",
        "\n",
        "else:\n",
        "    print(\"No validation graphs available to visualize.\")\n",
        "\n",
        "\n",
        "# --- Original visualization logic removed as it was causing OOM ---\n",
        "# The original code attempted to visualize a large concatenated graph,\n",
        "# which led to the OutOfMemoryError. Visualizing individual patches\n",
        "# or reassembling predictions on the original graph is necessary.\n",
        "# The code above now attempts to visualize the first full graph from its patches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b50d5ae1",
      "metadata": {
        "id": "b50d5ae1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ebf198a7c7224db78796dc992c254ad6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7117738377624b258f5e0553bb076dde",
              "IPY_MODEL_34d8a032ac2f493b8890e073361f217a",
              "IPY_MODEL_720ff6029503454dbe17ce48ee185171"
            ],
            "layout": "IPY_MODEL_c569f62b6ba44651bcd28795f55ff278"
          }
        },
        "7117738377624b258f5e0553bb076dde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_91243b41602a42f78630f86badf6efe4",
            "placeholder": "​",
            "style": "IPY_MODEL_490ce153e6b54745a063b6a2cc45493d",
            "value": "to_pyg: 100%"
          }
        },
        "34d8a032ac2f493b8890e073361f217a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f187208954a48cbbff3c99eb648fb63",
            "max": 120,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_891b6d2831144e4f8809440fb335cce7",
            "value": 120
          }
        },
        "720ff6029503454dbe17ce48ee185171": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d4e5df42d14457190dbae7d8e7849a3",
            "placeholder": "​",
            "style": "IPY_MODEL_2c8f3cca2f0848be913cd4ee4adc91ce",
            "value": " 120/120 [00:00&lt;00:00, 12519.38it/s]"
          }
        },
        "c569f62b6ba44651bcd28795f55ff278": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "91243b41602a42f78630f86badf6efe4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "490ce153e6b54745a063b6a2cc45493d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f187208954a48cbbff3c99eb648fb63": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "891b6d2831144e4f8809440fb335cce7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d4e5df42d14457190dbae7d8e7849a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c8f3cca2f0848be913cd4ee4adc91ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4b80160a83d649ba8e40e4e63b107971": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_84724253da914c439a94850fd8b5497f",
              "IPY_MODEL_b1f8d921ba3b47b48d45b4296f2c9909",
              "IPY_MODEL_61dddc2a244d4b879cf4bd8d6f29adbb"
            ],
            "layout": "IPY_MODEL_1089a4c9fe444710943be8e44aedbcf8"
          }
        },
        "84724253da914c439a94850fd8b5497f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd04420e04814f1285d9213e0b2507f9",
            "placeholder": "​",
            "style": "IPY_MODEL_96e29fbeb2d3469e84bad47d4fe4f2bb",
            "value": "to_pyg: 100%"
          }
        },
        "b1f8d921ba3b47b48d45b4296f2c9909": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58acb190438341b2889dc1e6df9143b5",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8db388040b724920a8f164fe64dc00da",
            "value": 30
          }
        },
        "61dddc2a244d4b879cf4bd8d6f29adbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86f1a8d258f74367b84a633441bdf99c",
            "placeholder": "​",
            "style": "IPY_MODEL_dfa469128d644cd3aeb4f8bf890faaaf",
            "value": " 30/30 [00:00&lt;00:00, 3633.63it/s]"
          }
        },
        "1089a4c9fe444710943be8e44aedbcf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd04420e04814f1285d9213e0b2507f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96e29fbeb2d3469e84bad47d4fe4f2bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58acb190438341b2889dc1e6df9143b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8db388040b724920a8f164fe64dc00da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "86f1a8d258f74367b84a633441bdf99c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfa469128d644cd3aeb4f8bf890faaaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}